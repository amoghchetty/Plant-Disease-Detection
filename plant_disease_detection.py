# -*- coding: utf-8 -*-
"""Plant Disease Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSnCuqUzFYTA544_Vp4rYxNg_poq1y8P
"""

!wget -O "dataset.zip" "https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/file_downloaded"

!unzip /content/dataset.zip

!ls /content/

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf

!pip install split-folders

import splitfolders
splitfolders.ratio('/content/Plant_leave_diseases_dataset_with_augmentation', output="dataset", seed=1337, ratio=(.8, .1,.1))

"""## **Data Preprocessing**"""

train_dir = "/content/dataset/train"
validation_dir = "/content/dataset/val"

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)

validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)
test_dir = "/content/dataset/test"
test_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

class_names = train_dataset.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

class_names

AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

preprocess_input = tf.keras.applications.efficientnet.preprocess_input

from google.colab import drive
drive.mount('/content/drive')

import os
import json
import splitfolders
splitfolders.ratio('/content/Plant_leave_diseases_dataset_with_augmentation', output="dataset_new", seed=1337, ratio=(.8, .1,.1))

base_data_directory = 'dataset_new/train'

try:
    class_names = sorted(os.listdir(base_data_directory))

    class_names = [name for name in class_names if not name.startswith('.')]

    class_indices = {str(i): name for i, name in enumerate(class_names)}

    print(f" Found {len(class_names)} class labels.")

except FileNotFoundError:
    print(f" ERROR: Could not find the split data folders at {base_data_directory}. Please check the folder name.")
    raise

LABELS_SAVE_PATH = "/content/drive/MyDrive/class_indices.json"

print(f"Saving class indices to: {LABELS_SAVE_PATH}")
with open(LABELS_SAVE_PATH, "w") as f:
    json.dump(class_indices, f, indent=4)

print(" class_indices.json saved successfully to Google Drive!")

"""## **Create the base model from the pre-trained convnets**"""

IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.EfficientNetB4(
    input_shape=IMG_SHAPE,
    include_top=False,
    weights='imagenet',
)

image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

"""## **Feature extraction**
**Freeze the convolutional base**
"""

base_model.trainable = False

base_model.summary()

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(len(class_names), activation='softmax')
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

inputs = tf.keras.Input(shape=(160, 160, 3))
x = preprocess_input(inputs)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

model.summary()

len(model.trainable_variables)

tf.keras.utils.plot_model(model, show_shapes=True)

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

initial_epochs = 6
loss0, accuracy0 = model.evaluate(validation_dataset)

print("initial loss: {:.2f}".format(loss0))
print("initial accuracy: {:.2f}".format(accuracy0))

history = model.fit(train_dataset,
                    epochs=initial_epochs,
                    validation_data=validation_dataset)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""## **Fine tuning**"""

base_model.trainable = True

print("Number of layers in the base model: ", len(base_model.layers))

fine_tune_at = 100

for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

model.summary()

len(model.trainable_variables)

fine_tune_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs

history_fine = model.fit(train_dataset,
                         epochs=total_epochs,
                         initial_epoch=len(history.epoch),
                         validation_data=validation_dataset)

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

loss, accuracy = model.evaluate(test_dataset)
print('Test accuracy :', accuracy)

image_batch, label_batch = test_dataset.as_numpy_iterator().next()
predictions = model.predict_on_batch(image_batch)
predictions = tf.argmax(predictions,axis=1)

print('Predictions:\n', predictions.numpy())
print('Labels:\n', label_batch)

plt.figure(figsize=(10, 10))
for i in range(9):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image_batch[i].astype("uint8"))
  plt.title(class_names[predictions[i]])
  plt.axis("off")

print("Final layer activation:", model.layers[-1].activation)

"""## **Saving Model**"""

model.save("plant_disease_recog_model_pwp.keras")

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

model.save("/content/drive/MyDrive/plant_disease_recog_model_pwp.keras")

import os

print("Checking if /content/dataset exists:", os.path.exists("/content/dataset"))

import os
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

base_path = "/content/dataset"
possible_test_dirs = ["test", "val", "validation"]
test_dir = None

for candidate in possible_test_dirs:
    candidate_path = os.path.join(base_path, candidate)
    if os.path.exists(candidate_path):
        test_dir = candidate_path
        break

if test_dir is None:
    raise FileNotFoundError("No valid test directory found in /content/dataset")

print(f" Using test directory: {test_dir}")

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

test_dataset = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE
)

# ðŸ”¹ Get labels & predictions
true_labels = []
pred_labels = []

for images, labels in test_dataset:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    true_labels.extend(labels.numpy())
    pred_labels.extend(preds)

print("\n=== Classification Report ===")
print(classification_report(true_labels, pred_labels, target_names=class_names))

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(20, 15))
sns.heatmap(cm, annot=False, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

import tensorflow as tf
import numpy as np

def safe_make_gradcam_heatmap(img_array_preprocessed, model, last_conv_layer):
    """
    Generates a Grad-CAM heatmap for a given image and model safely.
    Ensures model is built before computing gradients.
    """
    efficientnet_layer = model.get_layer("efficientnetb4")

    last_conv_layer = efficientnet_layer.get_layer("top_conv")


    if not model.built:
        _ = model(img_array_preprocessed)
    last_layer_activation = model.layers[-1].activation
    model.layers[-1].activation = None

    grad_model = tf.keras.models.Model(
        model.inputs, [last_conv_layer.output, model.output]
    )

    with tf.GradientTape() as tape:
        last_conv_layer_output, predictions = grad_model(img_array_preprocessed)
        predicted_class_index = tf.argmax(predictions[0])
        loss = predictions[:, predicted_class_index]

    grads = tape.gradient(loss, last_conv_layer_output)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0)
    max_val = tf.math.reduce_max(heatmap)
    if max_val == 0:
        heatmap = tf.zeros_like(heatmap)
    else:
        heatmap /= max_val

    model.layers[-1].activation = last_layer_activation

    return heatmap.numpy(), predicted_class_index.numpy()

!pip install -q streamlit pyngrok
from pyngrok import ngrok
ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import json
# import numpy as np
# import streamlit as st
# import tensorflow as tf
# from PIL import Image
# from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess
# 
# 
# MODEL_PATH = '/content/drive/MyDrive/plant_disease_recog_model_pwp.keras'
# LABELS_PATH = '/content/drive/MyDrive/class_indices.json'
# IMG_TARGET_SIZE = (160, 160)
# 
# st.set_page_config(page_title="Plant Disease Classifier", layout="centered")
# st.title(" Automated Plant Disease Detection using EfficientNetB4")
# 
# RECOMMENDATIONS = {
#     "Apple___Black_rot": (
#         "Detected: **Black Rot on Apple Leaves** \n\n"
#         " Cause: Caused by the fungus *Botryosphaeria obtusa*, leading to leaf spots and fruit rot.\n"
#         " Suggested Action: Remove and destroy affected leaves and mummified fruits. "
#         "Apply copper-based or sulfur fungicides early in the season. "
#         "Ensure good air circulation by pruning dense canopies."
#     ),
# 
#     "Apple___Cedar_apple_rust": (
#         "Detected: **Cedar Apple Rust Infection** \n\n"
#         " Cause: A fungal disease spread from nearby cedar or juniper trees.\n"
#         " Suggested Action: Remove nearby cedar hosts if possible. "
#         "Apply myclobutanil or propiconazole fungicides during early spring. "
#         "Maintain proper spacing and monitor after rainfall."
#     ),
# 
#     "Apple___Apple_scab": (
#         "Detected: **Apple Scab Disease** \n\n"
#         " Cause: Caused by the fungus *Venturia inaequalis*, leading to dark, velvety spots on leaves and fruits.\n"
#         " Suggested Action: Use resistant apple varieties if possible. "
#         "Remove fallen leaves after harvest. "
#         "Apply fungicides such as captan or mancozeb during budding and early fruit stages."
#     ),
# 
#     "Apple___healthy": (
#         "Detected: **Healthy Apple Leaf** \n\n"
#         "The leaf appears healthy and disease-free. "
#         "Maintain regular irrigation, monitor for pests, and provide balanced nutrition to sustain plant health."
#     ),
# 
#     "Tomato___Bacterial_spot": "Suggested Action: Apply copper-based fungicides. Use certified disease-free seeds/transplants.",
#     "Potato___Late_blight": "Suggested Action: Apply preventative fungicides, especially during cool, moist weather. Ensure proper plant spacing.",
#     "Background_without_leaves": "The image appears to be a background or non-leaf material. Please upload a clear leaf image."
# }
# 
# 
# @st.cache_resource(show_spinner=False)
# def load_model_and_labels(model_path: str, labels_path: str):
#     """Load the trained Keras model and class index mapping."""
#     if not os.path.exists(model_path):
#         st.error(f"Model file not found at {model_path}. Please check the path.")
#         st.stop()
#     if not os.path.exists(labels_path):
#         st.error(f"Labels file not found at {labels_path}. Please check the path.")
#         st.stop()
# 
#     model = tf.keras.models.load_model(model_path, compile=False)
#     with open(labels_path, "r") as f:
#         class_indices = json.load(f)
#     return model, class_indices
# 
# 
# def load_and_preprocess_image(image_file, target_size=IMG_TARGET_SIZE):
#     """Load, resize, and preprocess an image for EfficientNet."""
#     img = Image.open(image_file).convert("RGB")
#     display_img = img.resize((400, 400))
#     img = img.resize(target_size)
# 
#     img_array = np.array(img, dtype=np.float32)
# 
#     img_array = np.expand_dims(img_array, axis=0)
# 
#     img_array = efficientnet_preprocess(img_array)
# 
#     return img_array, display_img
# 
# 
# 
# 
# def predict_image_class(model, preprocessed_img_array, class_indices):
#     """Predict class label and compute confidence."""
# 
#     probs = model.predict(preprocessed_img_array, verbose=0)[0]
# 
#     predicted_class_index = int(np.argmax(probs))
#     predicted_class_name = class_indices.get(
#         str(predicted_class_index),
#         f"Unknown Class Index {predicted_class_index}"
#     )
# 
#     confidence = probs[predicted_class_index] * 100
#     confidence_display = np.round(confidence, 2)
# 
#     return predicted_class_name, confidence_display
# 
# 
# 
# 
# 
# load_state = st.empty()
# try:
#     with st.spinner("Loading EfficientNetB4 model and class indices..."):
#         model, class_indices = load_model_and_labels(MODEL_PATH, LABELS_PATH)
#     load_state.success(" Model loaded successfully!")
#     st.info(" Softmax layer added to normalize model outputs.")
# 
# except Exception as e:
#     load_state.error(f" Failed to load model or labels: {e}")
#     st.stop()
# 
# 
# st.subheader(" Upload a leaf image for disease detection")
# 
# uploaded_image = st.file_uploader(
#     "Upload Image",
#     type=["jpg", "jpeg", "png"],
#     help="Upload a clear image of the leaf"
# )
# 
# if uploaded_image is not None:
#     preprocessed_img_array, display_img = load_and_preprocess_image(uploaded_image)
# 
# 
#     st.image(display_img, caption="Uploaded Leaf Image", use_container_width=True)
# 
#     if st.button(" Classify"):
#         with st.spinner("Analyzing image and predicting disease..."):
#             prediction, confidence = predict_image_class(model, preprocessed_img_array, class_indices)
# 
# 
#         st.success(f" Detected Disease: **{prediction}**")
#         st.metric(label="Confidence", value=f"{confidence:.2f}%")
# 
# 
#         recommendation = RECOMMENDATIONS.get(
#             prediction,
#             "No specific recommendation found. Please consult a local agricultural expert."
#         )
# 
#         st.info(f"ðŸ’¡ **Recommendation:** {recommendation}")
# 
# else:
#     st.info("Please upload a leaf image to start the prediction.")
# 
#

from pyngrok import ngrok
import threading
import time
import os

ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")

def run_streamlit():
    os.system("streamlit run app.py")

thread = threading.Thread(target=run_streamlit)
thread.start()

time.sleep(5)

public_url = ngrok.connect(8501)
print(f"ðŸš€ Streamlit app is live at: {public_url}")

import tensorflow as tf

model = tf.keras.models.load_model('/content/drive/MyDrive/plant_disease_recog_model_pwp.keras', compile=False)
print(model.layers[-1].activation)

